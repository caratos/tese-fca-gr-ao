\chapter{Extração de Termos}
\label{chap:termos}

Um termo é uma unidade léxica que possui um significado concreto
dentro de um certo domínio \cite{term-cabre,term-krieger}. Ele pode
ser considerado também como uma expressão multipalavra (EMP), que é um
conjunto de palavras que possuem um significado proprio e cuja
frequencia em certo texto tem carácter idiossincrático, segundo
\cite{Sag01mwe}.

Para poder entender mais sobre EMPs, tomamos a definição de outros
elementos linguísticos: as \textit{expressões multipalavras}, também
chamadas \textit{sequencias formulaic}. Ela é definida por
\cite{formulaic} como: \textit{uma sequencia, continua ou descontinua,
  de palavras ou outros elementos, os quais são, ou parecem,
  pre-fabricados: isto é, armazenados e recuperados totalmente da
  memoria no momento do uso, ao invés de serem sujeitos de
  generalização ou análise pela gramática da linguagem.}


De acordo com \cite{Sag01mwe} existem dois tipos de EMP:

\begin{description}
\item[Frases léxicas], as quais possuem sintaxe ou semántica
  idiossincrática, ou contém 'palavras' que não ocorrem em
  isolamento. As expressões multipalavras deste tipo podem ser:
  \begin{itemize}
  \item Expressões idiomáticas, também chamadas expressões populares.
    Caracterizam-se por não ter um significado literal como por
    exemplo gírias: ``show de bola''.
  \item Compostos nominais, como ``centro estadual de educação
    tecnológica''.
  \item Nomes próprios, como ``Instituto de Matemática e Estatística''
  \item Construções verbais como ``dar é melhor que receber''.
  \item Verbos de suporte, como ``Tomar banho'' ou ``Poer em risco''.
  \end{itemize}
\item[Frases institucionalizadas], que são construções semânticas e
  sintáticas, mas estadisticamente idiossincráticas. Por exemplo em 
  ``banco de dados'', as palavras ``banco'' e ``dados'' possuem
  significado próprio, mas juntas possuim um significado próprio.
\end{description}


O trabalho de \cite{mwe}, propôs um método híbrido para identificar
EMPs a partir de documentos em inglês. Tal método usa processamento de
linguagem natural para extrair a informação gramatical de cada
palavra, e aplica uma técnica de alinhamento de sequencias para obter
aqueles segmentos mais similares entre todas elas.

Quando estamos esta técnica em um documento sobre métodos ágeis de
desenvolvimento de software, obtimos vários segmentos muito cumpridos,
como o seguinte:

\begin{verbatim}
Então se você tem um método que prevê , que se baseia em achar, da
para prever o futuro, então você já tem um problema em desenvolvimento  
de software ...
\end{verbatim}

Segmentos como o anterior foram encontrados em todo o texto, e isso
deveu-se a que muitas palavras, com pontuação elevada, estavam dentro
de tais segmentos, por exemplo \emph{método}, \emph{desenvolvimento} e
\emph{software}. 

Para reducir tais EMPs a simples termos, demandaria um grande esforço,
e sobretudo tempo o que não é justificável desde que existem outros
métodos mais diretos.

\section{Métodos para extração de termos}
A maioria de trabalhos usam técnicas estatísticas para extrair os
termos, por exemplo o trabalho de \cite{pantel01}, apresenta uma
métrica híbrida baseada no valor de informação mútua junto à
verossimilhança logarítmica.

O usso desses dois métodos está fundamentado devido que a
verossimilhança devolve valores altos para termos que acontecem como
erros, como por exemplo ``\textit{the the}'' no idioma inglês. Porém,
a informação mútua diminui a chance deste tipo de expressão ser
escolhida como termo.

Para melhorar os resultados de Pantel e Lin, o trabalho
\cite{keyphrase1} propôs o uso de uma fonte externa maior
(\emph{background}) para calcular as probabilidades com maior precisão
que usando apenas o texto a ser analisado (\emph{foreground}). Para
cada fonte foi criado um modelo de linguagem, que é uma estrutura de
dados que armazena probabilidades de ocorrências de uma palavra
(\emph{unigram}) ou várias palavras (\emph{N-gram}) apareceram no
texto.

Tomokiyo e Hurst usaram também o ponto de divergência \emph{KL}
(Kullback-Leibler) para medir a ineficácia de assumir uma certa
distribuição de probabilidade $p$ quando a verdadeira é $q$. A
divergência KL é conhecida também como entropia relativa.

Finalmente, o trabalho de \cite{phrasalterm} compara diferentes
métricas, a \emph{MutualRank} entre elas, e explica que os métodos
estatísticos não tem bons resultados devido à incorreta suposição da
distribuição de probabilidade (como por exemplo distribuição normal ou
de Poisson ), mas que a distribuição Zipf é a mais apropriada porque
ela se amolda melhor em distribuições assimétricas (skewed
distribution em inglês), as quais são mais comuns em ocorrências de
palavras.

A conclusão de Deane com respeito à incorreta suposição da
distribuição de probabilidade também é defendida pelo trabalho de Duan
et al. \cite{mwe}, que usou o alinhamento de sequencias na extração de
termos.


\section{Ferramentas para a extração de termos}
A seguir, na Tabela \ref{tab:ferramentas-extracao-termos},
apresentamos uma lista de ferramentas disponíveis na Internet,
dedicadas à extração de termos. Uma parte delas estão disponíveis como
aplicações desktop, e outra parte apenas estão para ser usados pela
Internet como um serviço de uma página web.

\begin{table}[htpb]
  \centering
  \begin{tabular}{|l|p{3cm}|p{10cm}|} \hline
    & \textbf{Linguagem de Programação} & \textbf{\textit{Descrição}} \\ \hline \hline

    \textbf{ExatoLP} & Java & Feita na Universidade de Rio Grande do Sul, esta
    ferramenta extrai termos a partir de textos escritos em português. Seu lançamento
    para baixar e usar livremente está anunciado desde o 2009. \\ \hline

    \textbf{KEA} & Java & Reconhece apenas palavras
    chaves em textos escritos em inglês. Devolve poucas palavras
    chave.  O trabalho de Lacerda \cite{kea-portugues} 
    adaptou este sistema para o português. \\ \hline

    \textbf{TOPIA} & Python & Código muito simples que compete contra o Yahoo Term
    Extraction. Usa um lexicon em inglês (varias palavras
    com anotações sintacticas). Esta ferramenta encontra-se incluída dentro de ZOPE, 
    que é uma biblioteca de aplicações em Python. Usa a métrica N/NC-Values. \\ \hline

    \textbf{NGRAMTSP} & Perl & É uma ferramenta mais estatística que linguística.
    Reconhecer N-Grams \\ \hline

    \textbf{MAUI} & Java & Suporta espanhol, francês e alemão; porém, ele consulta
    na Web para adquirir informação contextual. \\ \hline
  \end{tabular}
  \caption{Lista de ferramentas para extração de termos}
  \label{tab:ferramentas-extracao-termos}
\end{table}

Existem também sistemas online que realizam a tarefa de extração de
termos. Vemos eles a continuação na Tabela
\ref{tab:sistemas-online-termos}.

\begin{table}[htbp]
  \centering
  \begin{tabular}{|p{5cm}|p{10cm}|} \hline
    \textbf{Five Filter Term Extraction} & Online, muito bom para o inglês. 
    Usa o TOPIA \\ \hline

    \textbf{AchemyAPI} & Muito bom para o inglês. Ainda não está
    disponível para o português \\ \hline

    \textbf{ZEMANTA} & É um web-service. Uma versão cliente pode ser
    baixada  para realizar consultas em informações contextuais com 
    respeito dos textos que o usuario ingressa. \\ \hline

    \textbf{Yahoo's Term Extraction} & É um dos melhores. A quantidade de 
    consultas são restritas por dia \\ \hline

    \textbf{Terme Extractor} & É lento e trabala apenas com textos no inglês \\ \hline

    \textbf{Translated.net, Terminology Extraction} & Trabalha para o inglês, italiano 
    e francês.\\ \hline

    \textbf{TermMine} & Muito bom para o inglês. Ele tem uma versão para baixar e
    usar com o protègè.  Usa C-value e NC-value. Como lematizador usa
    o TreeTagger(pago) e o Genia Tagger \\ \hline

  \end{tabular}
  \caption{Lista de sistemas online para extração de termos}
  \label{tab:sistemas-online-termos}
\end{table}


\section{Reconhecimento de sintagmas nominais e preposicionais}
O Cogroo, apresentado em \cite{cogroo}, é uma projeto de código livre
para correção gramatical de textos em língua portuguesa. Ele possui
uma API que pode ser usada para realizar a lematização do corpus.

O projeto Cogroo usou o corpus da Floresta Sintática
\footnote{Floresta Sintática
  \url{http://www.linguateca.pt/floresta/corpus.html}} para treinar e
gerar modelos gramaticais. Tal corpus foi gerado pelo
\texttt{PALAVRAS}, que é uma ferramenta paga para a análise
gramatical.

Uma vantagem de usar o Cogroo é a desambiguação de frases, por exemplo
na sentença \textit{quem casa tem casa}, ele identifica positivamente
que a primeira ocorrência de \textit{casa} refere-se a um verbo e a
segunda ocorrência refere-se a um substantivo.

O Cogroo usa um \texttt{ShallowParser} para constroir uma árvore
sintática simple. A partir dessa árvore, pode-se obter uma lista de
tokens, cada um contendo a informação sintática (NP,SUB,etc) e/o
morfológica (Noun, third person, etc) das unidades gramaticais da
oração.

Por exemplo, na frase \textit{Os métodos ágeis são importantes no
  desenvolvimento de software}, o cogroo realizou uma análise
sintática, obtendo uma lista de tokens e uma árvore sintática que
apreciamos na Tabela \ref{tab:lematizacao} e na Figura
\ref{fig:arvore} respectivamente.

\begin{table}[htbp]
  \centering
  \begin{tabular}{|l|l|l|} \hline
   \textbf{Token} & \textbf{Lema} & \textbf{Tag Morfológico} \\ \hline
   Os & o  & determiner,male,plural \\ \hline
   métodos  & método  & noun,male,plural  \\ \hline
   ágeis & ágil  & adjective,male,plural  \\ \hline
   são  & ser  & verb,plural,third,present,indicative,finite  \\ \hline
   importantes  & importante  & adjective,male,plural  \\ \hline
   no  & em  & preposition  \\ \hline
   no  & o  & determiner,male,singular  \\ \hline
   desenvolvimento  & desenvolvimento  &  noun,male,singular  \\ \hline
   de  & de  & preposition  \\ \hline
   software  & software & noun,male,singular \\ \hline
  \end{tabular}
  \caption{Lematização e informação morfológica}
  \label{tab:lematizacao}
\end{table}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{Images/arvore.png}
  \caption{Árvore sintática retornada pelo Cogroo}
  \label{fig:arvore}
\end{figure}




\section{Extração de candidatos a termos com a métrica
  C-\textit{value}/NC-\textit{value}}

A métrica C-\textit{value}/NC-\textit{value} é prática pois precisa
das frequências das palavras, e também confiável já que vários
sistemas o usam, como o Copia, o Terme Extractor e OntoLP (plugin
desenvolvido no trabalho de \cite{ontolp}).

Esta métrica foi introduzada no trabalho \cite{frantzi98} e combina
informação linguística e estatística. O processo divide-se em duas
etapas: (i) Calcular os C-\textit{value}s e os NC-\textit{value}s.

\subsection{Calculando os C-\textit{values}}
Aqui calculamos um valor de ranking para os candidados de termos
usando apenas a frequencia das palavras no textno. Consiste em duas
partes:
\begin{enumerate}
\item Parte Linguistica:
  \begin{enumerate}
  \item Realiza-se um processo de lematização em todo o texto.
  \item Aplica-se um filtro linguístico para conseguir os sintagmas
    nominais e preposicionais.
  \item Remove-se os \textit{stop-words}.
  \end{enumerate}

\item Na parte estatística, retorna-se um valor para uma cadeia
  candidata a termo usando:
  \begin{enumerate}
  \item A frequencia total de ocurrências da cadeia candidata no
    corpus.
  \item A frequencia da cadeia candidata como parte de outros
    candidatos a termos maiores.
  \item O número desses candidatos maiores.
  \item O tamanho dos candidados (quantidade de palavras).
  \end{enumerate}
\end{enumerate}

Finalmente a métrica \textit{C-value} fica explícita da seguinte forma:
\begin{equation}
  C-value(a) = \left\{
    \begin{array}{l}
      log_2|a| \times f(a) \\
      log_2|a| \times (f(a) - \frac{1}{P(T_a)}\sum_{b \in T_a}f(b))
    \end{array}
  \right.
  \label{eq:c-value}
\end{equation}
onde:
\begin{description}
\item[$a$] é o candidato a termo,
\item[$f(.)$] é a frequencia de ocurrencia no corpus,
\item[$T_a$] é o conjunto de candidato a termos extraído que contém $a$,
\item[$P(T_a)$] é o número desses candidatos a termos.
\end{description}

\subsection{Calculando os NC-\textit{values}}
Uma vez calculado os C-\textit{values}, a informação contextual é
usada para ajudar na identificação dos termos corretos. O fundamento
de usar tal recurso é devido que, usualmente, podemos identificar o
significado de uma palavra analisando o contexto onde ela aparece.

Os tipos de palavras contextuais podem ser adjetivos, substantivos ou
verbos que estão próximos a um candidato a termo. A relevancia de uma
palavra contextual $weight(.)$:

\begin{equation}
  weight(w) = \frac{t(w)}{n}
  \label{eq:context-ranking}
\end{equation}
onde :
\begin{description}
\item[$w$] é a palavra contextual (substantivo, verbo ou adjetivo)
\item[$t(w)$] é o número de termos onde a palavra $w$ aparece junta.
\item[$n$] número total de termos considerados
\end{description}


Finalmente, temos:
\begin{equation}
  NC-value(a) = 0.8 \times C-value(a) + 0.2 \times \sum_{b \in C_a} f_a(b) weight(b)
  \label{eq:nc-value}
\end{equation}
onde:
\begin{description}
\item[$a$] é o termo candidato,
\item[$C_a$] é o conjunto de diferentes palavras contextuais de $a$
\item[$b$] é uma palavra de $C_a$,
\item[$f_a(b)$] é a frequencia de $b$ como uma palavra de contexto de $a$
\end{description}
